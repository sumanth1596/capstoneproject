# -*- coding: utf-8 -*-
"""Brand_Health_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NBqqMaSKgACmuNI5NOr9rQJoEPjxdawn

# Read Data
"""

import numpy as np
import pandas as pd
import seaborn as sns

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest
# importing ploting libraries
import matplotlib.pyplot as plt
import matplotlib.style
plt.style.use('classic')

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files

# Uploading the files
#upload = files.upload()

# Getting the external dataset
!gdown --id 1TalKoUQOI6Lfmo7Wf-GLvX-VHCRTmuDX

# read the data into df
df= pd.read_excel('uconn data.xlsx')

df_1=df.copy()

# to find the shape of the dataframe
df.shape

# to see if the data is loaded properly
df.head(5)

# find the frequency of sales per product ASIN
data= df.groupby(['Product Asin']).agg(frequency = ('Date','nunique'))
data

# sort the products in such a way most sold product asin appears first
data["frequency"].sort_values(ascending=False)

"""# Data Cleaning"""

df.info()

df.columns

df.columns=df.columns.str.strip() # removes all leading and trailing spaces in the column headers

df.head()

df.loc[:,('Country', 'Seller Id','Seller Type', 'Fulfillment By',
       'MAP Violation', 'Offer Strength', 'Product URL')]

df.loc[df["Discount %"].idxmax()]

df["Country"].unique()

df.isna().sum() # checking for missing values

(((df.isna().sum())/df.shape[0]).mul(100)).sort_values(ascending=False) # displays the % of data missing for each column

# Dropping Product Upc and Sku columns since they are not required for analysis and they have 50% missing values.
# Also dropping ASP and Marketplace links as they can be disregarded.

df=df.drop(['Product Upc','Product SKU','Marketplace Links','ASP','Product URL'],axis=1)

# Dropping the missing rows in MAP and Sellers

df = df.dropna(subset=['MAP', 'Sellers'])
df.info()

df.isna().sum()

# Replacing missing values in Discount column with 0 since it can mean no discount
# Since MAP is 0 for may rows that will lead to a infinity value for discount which is unrealistic.
# We impute 0 for discount where the values are missing.

#df['Discount %'] = df['Discount %'].fillna((df["Base Price"] - df["MAP"])/df["MAP"])
#df['Discount %'] = df['Discount %'].fillna(0)

# Filling missing country values with 'Missing' to prevent data loss

df["Country"]=df["Country"].fillna("Missing")

# Filling offer strength column with sensible values

df["Offer Strength"]=df["Offer Strength"].fillna("Unknown")

df.isna().sum() # we are only left with Discount to treat

df.info()

df[df.duplicated()]

df=df.sort_values(["Date"])

"""## MAP / Discount treatment"""

mb=df.loc[:,('Product Asin','Base Price')] #choose only product asin and base price

# compute median after grouping by product asin
mb1=mb.groupby('Product Asin').median()
mb1.reset_index(inplace=True)
mb1=mb1.rename(columns={'Base Price': 'Med Base Price'})
mb1

# this will give us the median base price for each product asin merged into original dataset df
df = pd.merge(df, mb1, on=['Product Asin'],how='left')

df.info()

df.isna().sum()

# Recalculating zero valued MAP to 70% of product level median base price

df['MAP']=np.where((df["MAP"]==0), 0.70*df['Med Base Price'], df['MAP'])

df[df['Base Price']<df['MAP']].shape

df[df['MAP Violation']=='Under MAP'].shape

"""As you can see, after recalculating MAP, some MAP values have gone above the base price. Hence we need to recategorize them from Over MAP to Under MAP"""

df[(df["MAP Violation"]=="Over MAP") & (df["Base Price"]<df["MAP"])].shape

# Recategorizing MAP Violation

df['MAP Violation']=np.where((df["MAP Violation"]=="Over MAP") & (df["Base Price"]<df["MAP"]), "Under MAP", df['MAP Violation'])

df[df['MAP Violation']=='Under MAP'].shape

#Treating missing values of discount now tha zero values of MAP are handled

df['Discount %'] = df['Discount %'].fillna((df["Base Price"] - df["MAP"])/df["MAP"])

df['Discount %'] = df['Discount %'].round(4)

df.isna().sum()

"""Now all the missing values have been treated with best methods.

# Graphical Univariate Analysis
"""

df.head()

#df.to_csv('Potoodata.csvâ€™)

#df.to_csv("C:\\Users\\SWATHI SUKUMAR\\Desktop\\Uconn_nbcu\\potoo.csv")

#df.to_csv('df.csv')
#files.download('df.csv')

df.dtypes

import matplotlib.pyplot as plt
import seaborn as sns

#create frequency table for 'Brand'
print("Number of Unique values", df['Brand'].nunique(),"\n")
df['Brand'].value_counts()

#create frequency table for 'Category'
print("Number of Unique values", df['Category'].nunique(),"\n")


df['Category'].value_counts().head(15).plot.bar()

# Histogram plot for Product Ratings
df['Product Rating'].plot.hist()

# Histogram plot for Number of Reviews
df['Number of Reviews'].plot.hist()

df['Product'].value_counts().head(15).plot.bar()

df['Sellers'].value_counts().head(15).plot.bar()

df['Country'].value_counts().head(15).plot.bar()

# Creating plot
plt.boxplot(df['Base Price'])

# Creating dataset for all the remaining columns
np.random.seed(10)

data_1 = df['Base Price']
data_2 = df['Shipping']
data_3 = df['Total Cost']
data_4 = df['MAP']
data = [data_1, data_2, data_3, data_4]

fig = plt.figure(figsize =(10, 7))

# Creating axes instance
ax = fig.add_axes([0, 0, 1, 1])

# Creating plot
bp = ax.boxplot(data)

# show plot
plt.show()

# Creating dataset for all the remaining columns
np.random.seed(10)

data_1 = df['Discount %']
data_2 = df['Units Sold']
data_3 = df['Stock']
data_4 = df['Total Sales']
data = [data_1, data_2, data_3, data_4]

fig = plt.figure(figsize =(10, 7))

# Creating axes instance
ax = fig.add_axes([0, 0, 1, 1])

# Creating plot
bp = ax.boxplot(data)

# show plot
plt.show()

df.info()

df.columns

# to understand the behaviour of fulfiment  by and MAP violation unvariate anlysis
df_columns=['Fulfillment By', 'MAP Violation']
for feature in df_columns:
        sns.countplot(x=df[feature]);
        plt.show()

# filter unknown from offer strength and plot the graph
df["Offer Strength"]

# this is a definition of color codes
class color:
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'

# desfriptive analysis of categories
column_cat=['Country', 'Seller Type',]
for i in column_cat:
    print(color.BOLD + i + color.END)
    print('\n')
    print(df[i].value_counts(normalize=True).mul(100).round(2).astype(str) + '%')
    print('\n')

print("There are ",df["Product"].nunique(), " products in the dataset")
print("There are ",df["Brand"].nunique(), " brands in the dataset")
print("There are ",df["Sellers"].nunique(), " sellers in the dataset")
print("There are ",df["Category"].nunique(), " categories in the dataset")
print("There are ",df["Country"].nunique(), " countries in the dataset")

df.select_dtypes(exclude='object').columns

df.columns

ncolumns=df.select_dtypes(exclude='object').columns

ncolumns

# calculate the top categories based on sales
top_categories= df.groupby(['Category']).agg(Sales = ('Total Sales','sum')).sort_values(by="Sales",ascending=False)
cat=top_categories.head(5)

cat.reset_index(inplace=True)

cat # top 5 categories based on sales

sns.set(rc = {'figure.figsize':(10,8)})
sns.barplot(x=cat["Category"],y=cat["Sales"],palette="flare")
plt.show()

ncol=['Product Rating', 'Number of Reviews', 'Base Price', 'Shipping','Total Cost', 'MAP', 'Discount %', 'Units Sold', 'Stock','Total Sales']

#univariate analysis of all numeric variables , to understand presence of outliers, distribution and skewness
for i in ncol:
  sns.distplot(df[i])
  plt.show()

for i in ncol:
  sns.boxplot(df[i])
  plt.show()

df.describe()

# find the dataset with baseprice below 0, negative
df[df['Base Price']<0]

# subset the data with discount is negative
dfd = df[df['Discount %']<0]
dfd

"""# Bivariate Analysis"""

df.head()

# Creating a matrix
df.corr()

#plot the correlation matrix of data dataframe.
fig, cmap = plt.subplots(figsize=(10,10))
sns.heatmap(df.corr(), annot=True, cmap = 'Reds')
plt.show()

# Cross Plots

plt.figure(figsize=(14, 14))

sns.pairplot(df, diag_kind='kde');

#plot the scatter plot of Discount % and Number of Reviews variable in data
plt.figure(figsize=(10,10))
plt.scatter(df['Discount %'],df['Number of Reviews'])
plt.title('Discount % vs Number of Reviews')
plt.xlabel('Discount %')
plt.ylabel('Number of Reviews')
plt.show()

#plot the scatter plot of MAP and Total Cost variable in data
plt.figure(figsize=(10,10))
plt.scatter(df['MAP'],df['Total Cost'])
plt.title('MAP vs Total Cost')
plt.xlabel('MAP')
plt.ylabel('Total Cost')
plt.show()

#plot the scatter plot of Product Rating Number of Reviews variable in data
plt.figure(figsize=(10,10))
plt.scatter(df['Product Rating'],df['Number of Reviews'])
plt.title('Product Rating vs Number of Reviews')
plt.xlabel('Product Rating')
plt.ylabel('Number of Reviews')
plt.show()

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
fig = go.Figure()

df_c = df[(df['Seller Type']=='AUTHORIZED') | (df['Seller Type']=='UNAUTHORIZED')]

fig = px.histogram (  df_c,
                      x="Fulfillment By",
                      color="Seller Type",
                      barnorm = "percent",
                      text_auto= True,
                      color_discrete_sequence=["mediumvioletred", "seagreen"],
                ) \
        .update_layout (

                    title={
                            "text": "Percent : Fullfillment - Seller Type",
                            "x": 0.5
                          },

                    yaxis_title="Percent"
                ) \
        .update_xaxes(categoryorder='total descending')

fig.show()

"""Over the years, as mentioned in the sponsor slides that 66 percent of sales are from Merchants whereas 33 percent from Amazon.

From the above graph we can see that the percentage of Unathorised sellers are more for Merchant than Amazon. The unathorised sellers may not be sticking to the pricing guidelines and  this might be one of the factor to be considered.
"""

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
fig = go.Figure()

df_e = df[(df['Offer Strength']=='Competitive') | (df['Offer Strength']=='Non Competitive')]

fig = px.histogram (  df_e,
                      x="Fulfillment By",
                      color="Offer Strength",
                      barnorm = "percent",
                      text_auto= True,
                      color_discrete_sequence=["mediumvioletred", "seagreen"],
                ) \
        .update_layout (

                    title={
                            "text": "Percent : Fullfillment - Offer Strength",
                            "x": 0.5
                          },

                    yaxis_title="Percent"
                ) \
        .update_xaxes(categoryorder='total descending')

fig.show()

"""From the above plot we can that Amazon are more competitive whereas the Merchant products are less Competetitive. We can also consider this factor while analysing the sales."""

# seller type vs map violation

fig = go.Figure()

df_d = df[(df['Seller Type']=='AUTHORIZED') | (df['Seller Type']=='UNAUTHORIZED')]

fig = px.histogram (  df_d,
                      x="Seller Type",
                      color="MAP Violation",
                      barnorm = "percent",
                      text_auto= True,
                      color_discrete_sequence=["mediumvioletred", "seagreen"],
                ) \
        .update_layout (

                    title={
                            "text": "Percent : Seller Type - MAP",
                            "x": 0.5
                          },

                    yaxis_title="Percent"
                ) \
        .update_xaxes(categoryorder='total descending')

fig.show()

"""# Outlier Treatment"""

# exclude all variables of type object
df_temp=df.select_dtypes(exclude='object')

df_temp.drop("Date",axis=1,inplace=True)

#compute the interquartile raange
Q1 = df_temp.quantile(0.25)
Q3 = df_temp.quantile(0.75)
IQR = Q3 - Q1
df_out=pd.DataFrame(((df_temp < (Q1 - 3 * IQR)) | (df_temp > (Q3 + 3 * IQR))).sum().sort_values(ascending=False))
df_out[df_out.iloc[:][0]!=0]

Q1

Q3

IQR

df[df["Number of Reviews"]==-1.0]

df[df["Base Price"] < 0]

"""There are negative number of reviews, and these are potential errors since number of reviews cannot be negative but can be 0. So we replaced them with 0.

There are datapoints that are having Base Price and Total cost as negatives. We assume that these values cannot be negatives and that they are potential errors.Hence we are removing them.

However, we would like to understand the root cause of why this has happened and what caused these values to occur from the business staand point
"""

#replace values that are negative with 0
df["Number of Reviews"]=np.where(df["Number of Reviews"]<0,0,df["Number of Reviews"])

df["Number of Reviews"].describe()

df[df["Base Price"]<0].count()

indexNames = df[df['Base Price'] <0 ].index
#find the index values for rows that has negative Base price

#Drop the negative base Price rows
df.drop(indexNames,inplace=True)

df.describe()

df[df["Shipping"]<0]["Units Sold"]

"""




Since the units sold for data points with shipping as negative values are 0, total sales is also 0. Negative values for shipping seems like an anamoly, Hence we must either remove them or repalce them with 0. Since we have only 9 datapoints, we can remove them."""

indexn = df[df['Shipping'] <0 ].index
#find the index values for rows that has negative Base price

#Drop the negative base Price rows
df.drop(indexn,inplace=True)

"""# Stock price vs Sales"""

# packages needed to call Yahoo finance API and load stocks
!pip install yahoo_fin
!pip install requests_html
import yahoo_fin.stock_info as si

# 1 : Amazon
from yahoo_fin.stock_info import get_data
amazon= get_data("AMZN", start_date="04/04/2022", end_date="05/03/2022", index_as_date = True, interval="1d")
print("The returned Amazon Data frame has ",amazon.shape[0],"rows and ",amazon.shape[1]," columns.")

amazon

# store adjusted closing price of amazon sales in variable
stocks=pd.DataFrame(amazon["adjclose"])

# create a datalist
datelist=pd.date_range(start="04-04-2022",end="05-02-2022")

# fill the missing data using backward fill, missing data is for weekend , will take the closing value of the monday
stocks=stocks.reindex(pd.date_range(stocks.index[0], stocks.index[-1], freq='D')).fillna(method='bfill')

stocks

sales= df.groupby(['Date']).agg(Tsales = ('Total Sales','sum'))
sales

plt.figure(figsize=(12,5))
plt.xlabel('Behaviour of Amazon Sales and Stock prices for April 2022')

ax1 = stocks["adjclose"].plot(color='black', grid=True, label='Stock prices of Amazon')
ax2 = sales["Tsales"].plot(color='red', grid=True, secondary_y=True, label='Total Sales of Amazon')

ax1.legend(loc=1)
ax2.legend(loc=2)

plt.show()

"""# Brand stocks vs Sales

###### identify the product most sold and retrieve stocks
"""

best_brand= df.groupby(['Brand']).agg(Sales = ('Total Sales','sum')).sort_values(by="Sales",ascending=False)
print(pd.DataFrame(best_brand.index))

# 2 : DEWALT SInce DeWalt is owned by Stanley black and Decker, we will use the stock ticker for stanley black and decker
from yahoo_fin.stock_info import get_data
dewalt= get_data("SWK", start_date="04/04/2022", end_date="05/03/2022", index_as_date = True, interval="1d")
print("The returned DeWalt Data frame has ",dewalt.shape[0],"rows and ",dewalt.shape[1]," columns.")

stock_d=pd.DataFrame(dewalt["adjclose"])

stock_d

stock_d=stock_d.reindex(pd.date_range(stock_d.index[0], stock_d.index[-1], freq='D')).fillna(method='bfill')

stock_d

sales= df[df["Brand"]=="DEWALT"].groupby(['Date']).agg(dsales = ('Total Sales','sum'))
sales

plt.figure(figsize=(12,5))
plt.xlabel('Behaviour of DEWALT Sales and their Stock prices for April 2022')

ax1 = stock_d["adjclose"].plot(color='brown', grid=True, label='Stock price of DEWALT')
ax2 = sales["dsales"].plot(color='green', grid=True, secondary_y=True, label='Total Sales of DEWALT')

ax1.legend(loc=1)
ax2.legend(loc=2)

plt.show()

df["Seller Type"].nunique()











"""# Brand Health

In Amazon,Products can be sold directly to the customer without authorization and without compliance to Brand guidelines. This has led to a lot of unauthorized sellers selling the products for a lower price, causing the Brands and retail partners to lose sales.

Let us analyze the sales difference between authorized and non-compliant sellers.

"""

tt=df.loc[:,('Seller Type','Date')]
t11=tt.groupby(['Seller Type']).count()
t11.reset_index(inplace=True)
t11=t11.rename(columns={'Date': 'Count'})
t11.sort_values(by=('Count'),ascending=False)

"""As you can observe, AUTHORIZED and UNAUTHORIZED occupy over 90% of the data. So let us neglect the remianing categories."""

#df_t=df.loc[:,('Brand','Seller Type','Total Sales')]

# subsetting the rows and selecting the 2 favoured categories of Seller type
options = ['AUTHORIZED', 'UNAUTHORIZED']

# selecting rows based on condition
dfa = df[df['Seller Type'].isin(options)]

# taking the required columns into a new dataframe
dfb=dfa.loc[:,('Brand','Seller Type','Total Sales')]

# Grouping the dataset based on Brand - Seller type

t1=dfb.groupby(['Brand','Seller Type']).sum().unstack() # Unstack helps make the seller type sub groups into columns
t1.reset_index(inplace=True)
t1

t1=t1.fillna(0) # filling the missing values with 0 because if there is no authorized seller then authorized seller contribution is 0

t1.isna().sum() # checking if the table is free of missing values

# simplyfying the multi index into a single level
t1=t1.droplevel(level=0, axis=1)

t1=t1.rename(columns={'': 'Brands'})

t1["sum"]=t1["AUTHORIZED"]+t1["UNAUTHORIZED"] # creating a sum column for sorting purpose

t1=t1.sort_values(by=('sum'),ascending=False) # sorting the table based on total sales in descending

t1

# taking the top 15 brands with max sales
t2=t1.head(15)
t2

"""Comparing the sales performance of the top 15 brands with most sales under partnered and other sellers."""

X = t2["Brands"]
Y1 = t2["AUTHORIZED"]
Y2 = t2["UNAUTHORIZED"]

X_axis = np.arange(len(X))

plt.bar(X_axis - 0.2, Y1, 0.4, label = 'AUTHORIZED')
plt.bar(X_axis + 0.2, Y2, 0.4, label = 'UNAUTHORIZED')


plt.xlabel("Brands")
plt.ylabel("Total Sales")
plt.title("AUTHORIZED v/s UNAUTHORIZED")
plt.rcParams["figure.figsize"] = (5,5)

plt.legend()
plt.show()

"""The above graph clearly depicts 8 out of 15 top brands have higher revenue contribution from unauthorized sellers.

Specifically analyzing the top 3 brands, the sales contribution from unauthorized sellers is many times more than the authorized sellers.

More than 93 % of DEWALT sales (total sales = 16 million) is contributed by unauthorized sellers.

The massive involvement of non-partnered sellers, selling at a price below guideline, forces the brands to match the price. This eventuals leads to the brands loosing their value and health.

We wanted to analyse the influence of Unauthorized sellers among all the brands in the dataset.

So we created a flag variable to flag the health of each brand(0 - Unhealthy,1 - Healthy) and set it as o if the number if unauthorized users for a brand is higher than the authorized ones and vice versa.
"""

# creating a flag variable health
# Setting health as 0 (meaning unhealthy) if unauthorized sellers are more than authorized (which makes sense) and vice versa.

t1['Health'] = np.where(t1['AUTHORIZED'] <= t1['UNAUTHORIZED'], 0, 1)
t1

# creating a histogram to check the frequency Healthy and unhealthy brands.

import seaborn as sns
plt.figure(figsize=(8, 5))
plt.locator_params(axis='x', nbins=1)
#plt.locator_params(axis='x', nbins=10)
sns.histplot(t1['Health'],stat="count") # creating a histogram based on the count of each category

plt.title("Brand Frequency")
plt.tight_layout()

plt.show()

"""# Multi variate"""

df.head()

# segregate brand that are top 5 based on sales
brand_df= df[df["Brand"].isin(["DEWALT","Bosch","Makita","BLACK & DECKER","Victorinox"])]

brand_df["Brand"].unique()

brand_df["Product"].unique()

ml_df=brand_df.groupby(["Seller Type","Offer Strength"]).agg(Max = ('Base Price','max'),Min= ('Base Price', "min"))
# grpup data based based on type pf seller to create rnage

ml_df.reset_index(inplace=True)

ml_df=ml_df[(ml_df["Offer Strength"]!="Unknown")]

ml_df["range"]=ml_df["Max"]-ml_df["Min"]

ml_df

# see how rnage of base price is changing acc seller type
sns.set(rc = {'figure.figsize':(15,5)})
#palette = sns.color_palette("hls")
sns.lineplot(x=ml_df["Seller Type"],y=ml_df["range"],hue=ml_df["Offer Strength"],palette="flare")

sns.set(rc = {'figure.figsize':(15,5)})
#palette = sns.color_palette("hls")
sns.lineplot(x=ml_df["Seller Type"],y=ml_df["Max"],hue=ml_df["Offer Strength"],palette="flare")

sns.set(rc = {'figure.figsize':(15,5)})
#palette = sns.color_palette("hls")
sns.lineplot(x=ml_df["Seller Type"],y=ml_df["Min"],hue=ml_df["Offer Strength"],palette="flare")

df["Product"].unique()







"""As you can see, more than 85 out of 102 brands are unhealthy which is nearly 84 %.

# Modeling
"""

df.head(5)

"""# APPROACH 1- Offer health

## Dropped approach


"""

###################################Offer Health###############################

#df['Offer Health'] = np.where((df["MAP Violation"]=="Under MAP") | (df["Base Price"]>df["Med Base Price"]), 1, 0)

"""# REVISED APPROACH- Offer health"""

df.columns

# create a new product asin and seller variable
df["prod_sell"]=(df["Product Asin"]).astype(str)+df["Sellers"]

# group by the key and find min and max
flagged_=df.groupby("prod_sell").agg(min=("Base Price","min"),max=("Base Price","max"))

# compute the rnage of the values
flagged_["range"]=flagged_["max"]-flagged_["min"]

# store median of rnage across all product & seller combo
med=flagged_["range"].median()

# create the flag that says values greater than median as 1 and rest as 0
flagged_["status_flag"]=np.where((flagged_["range"]>med), 1, 0)

flagged_.reset_index(inplace=True)

flagged_[flagged_["status_flag"]==1]

flagged_["range"].max()

flagged_[flagged_["range"]==14895.0]

#df_new.columns

sns.df_new[df_new["prod_sell"]=="B09HYD4YY7Amazon.com"]["Base Price"]

comp=df_new[df_new["prod_sell"]=="B09HYD4YY7Amazon.com"]

comp["Base Price"]

sns.set(rc = {'figure.figsize':(8,8)})
#palette = sns.color_palette("hls")
sns.lineplot(x=comp["Date"],y=comp["Base Price"],palette="flare")

x = flagged_[["prod_sell","status_flag"]]

# merge the offer health variable with the original dataset
df_new=df.merge(x,left_on="prod_sell",right_on="prod_sell",how="inner")

df_new

df_new["status_flag"].value_counts(normalize=True)

df[df["Offer Health"]==1]

from google.colab import files

df_new.to_csv('sample.csv') # import file to local

files.download("sample.csv")

#with MAP, derive discount values (infinity since we had MAP hhad 0s)-imouting MAP with second least value != 0
#how can we alter MAP so that we dont get an infinite discounts
#How can discounts impact the health of the brand ( the bigger the discounts, the chances of losing money is high)
#see if we can create dummy variables than can be fed to the model.
#summary stats- how big the variation in MAP price vs base price ( insights on imputation )
# dont rule out the MAP column, and the MAP values could be a client imputed number at the present.

topb=df.groupby("Brand").agg(totals=("Total Sales","sum")).sort_values(by="totals",ascending=False)

topb.head(10)

#sklearn has lot of supervised learning algortihms but we are considering only lineardiscriminant analysis here
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import scale

# Import Logistic Regression machine learning library
from sklearn.linear_model import LogisticRegression

#Let us break the X and y dataframes into training set and test set. For this we will use
#Sklearn package's data splitting function which is based on random function

from sklearn.model_selection import train_test_split,GridSearchCV

import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix

df.columns

# Import label encoder
from sklearn import preprocessing
# Get one hot encoding of columns B where B is thee column of interest
one_hot = pd.get_dummies(df_new['Fulfillment By'])
# Drop column B as it is now encoded
df_new = df_new.drop('Fulfillment By',axis = 1)
# Join the encoded df
df_new = df_new.join(one_hot)

df_new.drop("MERCHANT",axis=1, inplace=True)

df_new.columns

# Get one hot encoding of columns B
one_hot = pd.get_dummies(df_new['MAP Violation'])
# Drop column B as it is now encoded
df_new = df_new.drop('MAP Violation',axis = 1)
# Join the encoded df
df_new = df_new.join(one_hot)
df_new.drop("Over MAP",axis=1, inplace=True)

#df.drop("MAP Violation",axis=1,inplace=True)

# Get one hot encoding of columns B
one_hot = pd.get_dummies(df_new['Offer Strength'])
# Drop column B as it is now encoded
df_new = df_new.drop('Offer Strength',axis = 1)
# Join the encoded df
df_new = df_new.join(one_hot)
df_new.drop("Non Competitive",axis=1, inplace=True)

df_new.columns

df1=df_new.drop(['Date', 'Product Asin', 'Brand', 'Category', 'Product', 'Sellers', 'Country', 'Seller Id','Seller Type',"MAP","prod_sell","Unknown"],axis=1)

#df1.drop(["Fulfillment By"],axis=1,inplace=True)

df1.drop("Med Base Price",axis=1,inplace=True)

df1.columns

#df1['MAP'] = df1['MAP'].fillna(0)

#X= df1.drop("Offer Health",axis=1)
#y=df1["Offer Health"]

X= df1.drop("status_flag",axis=1)
y=df1["status_flag"]



"""# Logistic Regression"""

df_new["status_flag"].value_counts(normalize=True)

# Split X and y into training and test set in 70:30 ratio
from sklearn.model_selection import train_test_split
X_res, X_test, y_res, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)

X_res.columns

sm = SMOTE(random_state=22)
X_res, y_res = sm.fit_resample(X_train, y_train)

X_train.info()

df1.info()

# Fit the Logistic Regression model
model = LogisticRegression(solver='newton-cg',max_iter=1000,penalty='l2',verbose=True,n_jobs=2)
model.fit(X_res, y_res)

ytrain_predict = model.predict(X_train)
ytest_predict = model.predict(X_test)

ytrain_predict = model.predict(X_res)
ytest_predict = model.predict(X_test)



ytest_predict_prob=model.predict_proba(X_test)
pd.DataFrame(ytest_predict_prob).head()

# Accuracy - Training Data
model.score(X_train, y_train)

# Accuracy - Training Data
model.score(X_res, y_res)

# Accuracy - test Data
model.score(X_test, y_test)



# predict probabilities
probs = model.predict_proba(X_res)
# keep probabilities for the positive outcome only
probs = probs[:, 1]
# calculate AUC
auc = roc_auc_score(y_res, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
train_fpr, train_tpr, train_thresholds = roc_curve(y_res, probs)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(train_fpr, train_tpr);

# predict probabilities
probs = model.predict_proba(X_test)
# keep probabilities for the positive outcome only
probs = probs[:, 1]
# calculate AUC
test_auc = roc_auc_score(y_test, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
test_fpr, test_tpr, test_thresholds = roc_curve(y_test, probs)
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(test_fpr, test_tpr);

print(classification_report(y_test, ytest_predict))

df1.columns

df1.columns=['Product_Rating', 'Number_of_Reviews', 'Base_Price', 'Shipping','Total_Cost', 'Discount', 'Units_Sold', 'Stock', 'Total_Sales', 'status_flag', "Amazon","under_map","Competitive"]

import statsmodels.formula.api as smf

# this is to get the summary stats from logistic regression appproach

result_l = smf.logit(formula='status_flag ~ Product_Rating+Number_of_Reviews+Base_Price+Shipping+Discount+Total_Cost+Stock+Total_Sales+Units_Sold+Amazon+Competitive', data=df1).fit()

result_l.summary()

#To Do: Deadline - Tomorrow EOD

## MAP or discount (any one of it) imputation --- Gowtham
## Offer Health relationship with other variables like brand, category, sellers, product (top 10 if unique values area a lot) (find a % of values acorss each category of offer health)-- Sumanh
## LDA--Swathi
## Random Forest (feature importance)-- Yogesh
## Presentations--- Harshitha



"""# LDA"""

#Build LDA Model
clf = LinearDiscriminantAnalysis()
model=clf.fit(X_train,y_train)

# Training Data Class Prediction with a cut-off value of 0.5
pred_class_train = model.predict(X_train)

# Test Data Class Prediction with a cut-off value of 0.5- default is 0.5
pred_class_test = model.predict(X_test)

from sklearn import metrics

f,a =  plt.subplots(1,2,sharex=True,sharey=True,squeeze=False)

#Plotting confusion matrix for the different models for the Training Data

plot_0 = sns.heatmap((metrics.confusion_matrix(y_train,pred_class_train)),annot=True,fmt='.5g',cmap='Greys',ax=a[0][0]);
a[0][0].set_title('Training Data')

plot_1 = sns.heatmap((metrics.confusion_matrix(y_test,pred_class_test)),annot=True,fmt='.5g',cmap='Greys',ax=a[0][1]);
a[0][1].set_title('Test Data');

print('Classification Report of the training data:\n\n',metrics.classification_report(y_train,pred_class_train),'\n')
print('Classification Report of the test data:\n\n',metrics.classification_report(y_test,pred_class_test),'\n')

# Training Data Probability Prediction
pred_prob_train = model.predict_proba(X_train)

# Test Data Probability Prediction
pred_prob_test = model.predict_proba(X_test)

# AUC and ROC for the training data

# calculate AUC
auc = metrics.roc_auc_score(y_train,pred_prob_train[:,1])
print('AUC for the Training Data: %.3f' % auc)

#  calculate roc curve
fpr, tpr, thresholds = metrics.roc_curve(y_train,pred_prob_train[:,1])
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.',label = 'Training Data')


# AUC and ROC for the test data

# calculate AUC
auc = metrics.roc_auc_score(y_test,pred_prob_test[:,1])
print('AUC for the Test Data: %.3f' % auc)

#  calculate roc curve
fpr, tpr, thresholds = metrics.roc_curve(y_test,pred_prob_test[:,1])
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.',label='Test Data')
# show the plot
plt.legend(loc='best')
plt.show()

#for j in np.arange(0.1,1,0.1):
 #   custom_prob = j #defining the cut-off value of our choice
  #  custom_cutoff_data=[]#defining an empty list
   # for i in range(0,len(y_train)):#defining a loop for the length of the train data
    #    if np.array(pred_prob_train[:,1])[i] > custom_prob:#issuing a condition for our probability values to be
     #       #greater than the custom cutoff value
      #      a=1#if the probability values are greater than the custom cutoff then the value should be 1
       # else:
        #    a=0#if the probability values are less than the custom cutoff then the value should be 0
        #custom_cutoff_data.append(a)#adding either 1 or 0 based on the condition to the end of the list defined by us
    #print(round(j,3),'\n')
   # print('Accuracy Score',round(metrics.accuracy_score(y_train,custom_cutoff_data),4))
    #print('F1 Score',round(metrics.f1_score(y_train,custom_cutoff_data),4),'\n')
    #plt.figure(figsize=(6,4))
    #print('Confusion Matrix')
    #sns.heatmap(metrics.confusion_matrix(y_train,custom_cutoff_data),annot=True,fmt='.4g'),'\n\n'
    #plt.show();

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
#rfcl = RandomForestClassifier(n_estimators = 100, random_state=1,max_features=7)
#rfcl = rfcl.fit(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier

rfcl1 = RandomForestClassifier(n_estimators = 50, random_state=1,max_features=5,min_samples_leaf=50)
rfcl1 = rfcl1.fit(X_res, y_res)

ytrain_predict = rfcl1.predict(X_res)
ytest_predict = rfcl1.predict(X_test)

#confusion_matrix(y_train, ytrain_predict)

#Train Data Accuracy
rf_train_acc=rfcl1.score(X_res,y_res)
rf_train_acc

print(classification_report(y_res, ytrain_predict))

confusion_matrix(y_test, ytest_predict)

print(classification_report(y_test, ytest_predict))

# Training Data Probability Prediction
pred_prob_train = rfcl1.predict_proba(X_res)

# Test Data Probability Prediction
pred_prob_test = rfcl1.predict_proba(X_test)

# AUC and ROC for the training data

# calculate AUC
auc = metrics.roc_auc_score(y_train,pred_prob_train[:,1])
print('AUC for the Training Data: %.3f' % auc)

#  calculate roc curve
fpr, tpr, thresholds = metrics.roc_curve(y_train,pred_prob_train[:,1])
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.',label = 'Training Data')


# AUC and ROC for the test data

# calculate AUC
auc = metrics.roc_auc_score(y_test,pred_prob_test[:,1])
print('AUC for the Test Data: %.3f' % auc)

#  calculate roc curve
fpr, tpr, thresholds = metrics.roc_curve(y_test,pred_prob_test[:,1])
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.',label='Test Data')
# show the plot
plt.legend(loc='best')
plt.show()

param_grid = {
    'max_features': [4,5],
    'min_samples_leaf': [50,60],
    'n_estimators': [50]
}

rfcl = RandomForestClassifier()

grid_search = GridSearchCV(estimator = rfcl, param_grid = param_grid, cv = 5)

grid_search.fit(X_res, y_res)
print(grid_search.best_params_)
best_grid = grid_search.best_estimator_
best_grid

ytrain_predict = best_grid.predict(X_train)
ytest_predict = best_grid.predict(X_test)

confusion_matrix(y_train, ytrain_predict)

#Train Data Accuracy
rf_train_acc=best_grid.score(X_train,y_train)
rf_train_acc

confusion_matrix(y_test, ytest_predict)

#Train Data Accuracy
rf_test_acc=best_grid.score(X_test,y_test)
rf_test_acc

# Training Data Probability Prediction
pred_prob_train = best_grid.predict_proba(X_train)

# Test Data Probability Prediction
pred_prob_test = best_grid.predict_proba(X_test)

# AUC and ROC for the training data

# calculate AUC
auc = metrics.roc_auc_score(y_train,pred_prob_train[:,1])
print('AUC for the Training Data: %.3f' % auc)

#  calculate roc curve
fpr, tpr, thresholds = metrics.roc_curve(y_train,pred_prob_train[:,1])
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.',label = 'Training Data')


# AUC and ROC for the test data

# calculate AUC
auc = metrics.roc_auc_score(y_test,pred_prob_test[:,1])
print('AUC for the Test Data: %.3f' % auc)

#  calculate roc curve
fpr, tpr, thresholds = metrics.roc_curve(y_test,pred_prob_test[:,1])
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.',label='Test Data')
# show the plot
plt.legend(loc='best')
plt.show()

importances = rfcl1.feature_importances_
forest_importances = pd.Series(importances)
forest_importances

X_res.columns

labels = ['Product Rating', 'Number of Reviews', 'Base Price', 'Shipping',
       'Total Cost', 'Discount %', 'Units Sold', 'Stock', 'Total Sales', 'AMAZON', 'Under Map','Competitive']
fig, ax = plt.subplots()
plt.figure(figsize=(10,20))
forest_importances.plot.bar(ax=ax)
ax.set_title("Feature importances")
ax.set_ylabel("Mean decrease in impurity")
ax.set_xticklabels(labels, rotation = 50)

df.columns

